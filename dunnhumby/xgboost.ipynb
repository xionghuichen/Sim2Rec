{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import sys\n",
    "from src.utils import create_folder\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting, space_eval\n",
    "from src.utils import *\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "proj_path = Path.cwd()\n",
    "with open(os.path.join(proj_path, 'catalog.yml'), \"r\") as f:\n",
    "    catalog = yaml.safe_load(f)['breakfast']\n",
    "    \n",
    "with open(os.path.join(proj_path, 'params.yml'), \"r\") as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Search\n",
    "space = {\n",
    "    'eta': hp.quniform('eta', 0.02, 0.5, 0.01),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.2, 1, 0.1),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.2, 1, 0.1),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(5, 150, dtype=int))\n",
    "}\n",
    "\n",
    "def optimize():\n",
    "    \n",
    "    best = fmin(_score, space, algo=tpe.suggest, trials=trials, max_evals=100, verbose=0)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv('data/processed/merged_data.csv')\n",
    "merged_data['WEEK_END_DATE'] = pd.to_datetime(merged_data['WEEK_END_DATE'])\n",
    "original_data = merged_data.copy()\n",
    "merged_data['WEEK_END_DATE'] = merged_data['WEEK_END_DATE'] + timedelta(days=3)\n",
    "data_ranges = make_dates(params['breakfast']['experiment_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2277, 1600027527), (2277, 3800031838), (2277, 1111009477), (2277, 7192100339), (389, 1600027527), (389, 3800031838), (389, 1111009477), (389, 7192100339), (25229, 1600027527), (25229, 3800031838), (25229, 1111009477), (25229, 7192100339)]\n"
     ]
    }
   ],
   "source": [
    "stores = list(params['breakfast']['dataset']['store_ids'].keys())\n",
    "upcs = list(params['breakfast']['dataset']['upc_ids'].keys())\n",
    "import itertools\n",
    "store_upc_pairs = list(itertools.product(stores, upcs))\n",
    "print(store_upc_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "#for store_id, upc_id in store_upc_pairs:\n",
    "#    create_folder(os.path.join(proj_path, 'runs'))\n",
    "for _, train_start, train_end, valid_start, valid_end, test_start, test_end in date_ranges.itertuples():\n",
    "    lag_units = params['xgb']['window_size']\n",
    "    avg_units = params['xgb']['avg_units']\n",
    "    #control features\n",
    "\n",
    "    #filtered_data = merged_data[merged_data['ADDRESS_STATE_PROV_CODE']=='TX'][['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS', 'PRICE', 'BASE_PRICE', 'DESCRIPTION', 'MANUFACTURER', 'CATEGORY', 'SUB_CATEGORY', 'PRODUCT_SIZE', 'STORE_ID', 'STORE_NAME', 'ADDRESS_CITY_NAME', 'MSA_CODE', 'SEG_VALUE_NAME', 'PARKING_SPACE_QTY', 'SALES_AREA_SIZE_NUM', 'AVG_WEEKLY_BASKETS']].copy()\n",
    "    filtered_data = merged_data[merged_data['ADDRESS_STATE_PROV_CODE']=='TX'][['WEEK_END_DATE', 'STORE_NUM', 'UPC', 'UNITS', 'PRICE', 'BASE_PRICE', 'STORE_ID', 'MSA_CODE', 'PARKING_SPACE_QTY', 'SALES_AREA_SIZE_NUM', 'AVG_WEEKLY_BASKETS']].copy()\n",
    "\n",
    "    #Filter data\n",
    "    make_lag_features(filtered_data, lag_units, col_name='UNITS', prefix_name='lag-units', inplace=True)\n",
    "    make_historical_avg(filtered_data, r_list=avg_units, col_n='lag-units-1', google_trends=True)\n",
    "    add_datepart(filtered_data, fldname='WEEK_END_DATE', drop=False)\n",
    "\n",
    "    training_df = filtered_data[(filtered_data['WEEK_END_DATE']>=train_start) & (filtered_data['WEEK_END_DATE']<=train_end)].copy()\n",
    "    valid_df = filtered_data[(filtered_data['WEEK_END_DATE']>=valid_start) & (filtered_data['WEEK_END_DATE']<=valid_end)].copy()\n",
    "    test_df = filtered_data[(filtered_data['WEEK_END_DATE']>=test_start) & (filtered_data['WEEK_END_DATE']<=test_end)].copy()\n",
    "\n",
    "    train_df.set_index('WEEK_END_DATE', inplace=True)\n",
    "    valid_df.set_index('WEEK_END_DATE', inplace=True)\n",
    "    test_df.set_index('WEEK_END_DATE', inplace=True)\n",
    "\n",
    "    X_train = training_df\n",
    "    y_train = X_train.pop('UNITS')\n",
    "    X_valid = valid_df\n",
    "    y_valid = X_valid.pop('UNITS')\n",
    "    X_test = test_df\n",
    "    y_test = X_test.pop('UNITS')\n",
    "\n",
    "    #Function used to perform an evaluation on the validation and return the score to the trained model\n",
    "    def _score(params):\n",
    "        xg_boost_model = xgb.XGBRegressor(objective = 'reg:squarederror',\n",
    "                                        colsample_bytree = params['colsample_bytree'],\n",
    "                                        learning_rate = params['eta'],\n",
    "                                        max_depth = params['depth'],\n",
    "                                        min_child_weight = params['min_child_weight'],\n",
    "                                        n_estimators = params['n_estimators'],\n",
    "                                        random_state = 2020,\n",
    "                                        subsample = params['subsample'],\n",
    "                                        tree_method = 'hist')\n",
    "        xg_boost_model.fit(X_train, y_train)\n",
    "        preds = xg_boost_model.predict(X_valid)\n",
    "        mape = mean_absolute_percentage_error(y_valid, preds)\n",
    "        return mape\n",
    "\n",
    "    trials = Trials()\n",
    "    best_hyperparams = optimize()\n",
    "    hyperparameters = space_eval(space, best_hyperparams)\n",
    "    xgb_model = XGBClassifier(hyperparameters)\n",
    "    xgb_model.fit(pd.concat([X_train, X_valid]), pd.concat([y_train, y_valid]))\n",
    "    \n",
    "    test_preds = xgb_model.predict(X_test)\n",
    "    #test_metrics = get_metrics(y_test.values, test_preds)\n",
    "    test_df['test_predictions'] = test_preds\n",
    "    test_df['y_true'] = y_test.values\n",
    "\n",
    "    fname = './results/' + 'xgb_TX.csv'\n",
    "    test_df.to_csv(fname)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46155de7f424a33b1013700d1b1165e8a235e7bade17d644e8c35c01b956e3ba"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
